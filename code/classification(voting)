# import
import re
import torch
import numpy as np
import pandas as pd
import torch.optim as optim
import torch.nn.functional as F
import evaluate
from datasets import Dataset
from torch.utils.data import Dataset, DataLoader
from torch import nn
from tqdm import tqdm
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertModel
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, EarlyStoppingCallback
from transformers import DataCollatorWithPadding
from transformers import AutoModel, AutoTokenizer



# data 불러오기
data = pd.read_excel("daum_news_crawling(new).xlsx", index_col = 0, engine = 'openpyxl')



# 보수/진보 메이저 언론사 분류
p_data = data[(data['source'] == '조선일보') | (data['source'] == '중앙일보') | (data['source'] == '동아일보')]
p_data = p_data.reset_index()

n_data = data[(data['source'] == '한겨레') | (data['source'] == '경향신문')]
n_data = n_data.reset_index()



# 각종 전처리
p_data_merge = p_data.drop_duplicates(subset = 'title').reset_index(drop = True)
p_data_merge = p_data.drop_duplicates(subset = 'contents').reset_index(drop = True)

n_data_merge = n_data.drop_duplicates(subset = 'title').reset_index(drop = True)
n_data_merge = n_data.drop_duplicates(subset = 'contents').reset_index(drop = True)

p_data_final = p_data_merge.loc[:, ['title', "contents"]]
p_data_final['label'] = 0

p_data_final['title'] = p_data_final['title'].fillna("")
p_data_final['contents'] = p_data_final['contents'].fillna("")

n_data_final = n_data_merge.loc[:, ['title', "contents"]]
n_data_final['label'] = 1

tqdm.pandas()

def clean_text(texts):
    review = texts
    review = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\♣\▲\ⓒ\■\[\]\“\”\☞\‘\’\▶\·\…\〃\<\>"]', '', review)
    review = re.sub(r'\S+\s+기자', '', review)
    review = re.sub(r'\S+\s+선임기자', '', review)
    review = re.sub(r'\S+\s+군사전문기자', '', review)
    review = re.sub(r'\S+기자', '', review)
    review = re.sub(r'\S+선임기자', '', review)
    review = re.sub(r'\S+군사전문기자', '', review)
    review = re.sub(r'\s+', ' ', review)
    review = re.sub(r'^\s+', '', review)
    review = re.sub(r'\s+$', '', review)
    return review

p_data_final['clean_contents'] = p_data_final['contents'].progress_apply(clean_text)
n_data_final['clean_contents'] = n_data_final['contents'].progress_apply(clean_text)
p_data_final['title'] = p_data_final['title'].progress_apply(clean_text)
n_data_final['title'] = n_data_final['title'].progress_apply(clean_text)

pattern = r'\<.*?\>|\[.*?\]'

p_data_final['clean_contents'] = p_data_final['clean_contents'].apply(lambda x: re.sub(pattern, '', x))
n_data_final['clean_contents'] = n_data_final['clean_contents'].apply(lambda x: re.sub(pattern, '', x))
p_data_final['title'] = p_data_final['title'].apply(lambda x: re.sub(pattern, '', x))
n_data_final['title'] = n_data_final['title'].apply(lambda x: re.sub(pattern, '', x))

remove_source = ['조선일보', '중앙일보', '동아일보', '한겨레', '경향신문', '오마이뉴스', '동아닷컴', '뉴시스', '중앙포토', '연합뉴스',
                 '무단 전재 및 재배포 금지', '기사', '어제 못본', '명장면이 궁금하다면', '오늘의', '절친이 되어 주세요', '코인데스크', 
                 '신문구독주주신청', '페이스북카카오톡사설칼럼신문', '무단전재 및 재배포 금지', '디지털뉴스팀', '팩트체크페이스', '구독', 
                 '박태근', '박성진', '유신모', '김재중', '박은경', '정욱식', '도쿄박형준', '박지훈', '최현호', 
                 '김진우', '박영환', '윤상호', '김민석', '박병수', '박민규', '이상훈', '김상호']

pattern = '|'.join(remove_source)
p_data_final['clean_contents'] = p_data_final['clean_contents'].str.replace(pattern, '', regex = True)
n_data_final['clean_contents'] = n_data_final['clean_contents'].str.replace(pattern, '', regex = True)

sum_data_final = pd.concat([p_data_final, n_data_final], axis=0)
sum_data_final.reset_index(inplace=True)

sum_data_final = sum_data_final[sum_data_final['title'].str.len() >= 10]



# 모델 학습 및 평가
model_name = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

label_dict = {}
for i, label in enumerate(set(sum_data_final["label"])):
    label_dict[label] = i

sum_data_final = sum_data_final[["title", "clean_contents", "label"]]
for i in label_dict:
    sum_data_final.loc[(sum_data_final['label'] == i), 'label'] = label_dict[i]

title_train_data, title_test_data = train_test_split(sum_data_final, test_size=0.2, random_state=42)
title_val_data, title_test_data = train_test_split(title_test_data, test_size=0.5, random_state=42)

title_train_data.rename(columns={'title': 'context'}, inplace=True)
title_val_data.rename(columns={'title': 'context'}, inplace=True)
title_test_data.rename(columns={'title': 'context'}, inplace=True)

title_train_data = title_train_data[["context", "label"]]
title_val_data = title_val_data[["context", "label"]]
title_test_data = title_test_data[["context", "label"]]

contents_train_data, contents_test_data = train_test_split(sum_data_final, test_size=0.2, random_state=42)
contents_val_data, contents_test_data = train_test_split(contents_test_data, test_size=0.5, random_state=42)
contents_train_data.rename(columns={'clean_contents': 'context'}, inplace=True)
contents_val_data.rename(columns={'clean_contents': 'context'}, inplace=True)
contents_test_data.rename(columns={'clean_contents': 'context'}, inplace=True)

contents_train_data = contents_train_data[["context", "label"]]
contents_val_data = contents_val_data[["context", "label"]]
contents_test_data = contents_test_data[["context", "label"]]

title_train_dataset = Dataset.from_pandas(title_train_data)
title_val_dataset = Dataset.from_pandas(title_val_data)
title_test_dataset = Dataset.from_pandas(title_test_data)

contents_train_dataset = Dataset.from_pandas(contents_train_data)
contents_val_dataset = Dataset.from_pandas(contents_val_data)
contents_test_dataset = Dataset.from_pandas(contents_test_data)

title_train_dataset = title_train_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)
title_val_dataset = title_val_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)
title_test_dataset = title_test_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)

contents_train_dataset = contents_train_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)
contents_val_dataset = contents_val_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)
contents_test_dataset = contents_test_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)

training_args = TrainingArguments(
    output_dir="BERT_result_title",
    learning_rate=1e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=10,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

training_args2 = TrainingArguments(
    output_dir="BERT_result_contents",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=10,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

device = torch.device("cuda")

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
title_model = BertForSequenceClassification.from_pretrained("klue/bert-base", num_labels=2).to(device)

title_trainer = Trainer(
    model=title_model.to(device),
    args=training_args,
    train_dataset=title_train_dataset,
    eval_dataset=title_val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

title_trainer.train()

title_predictions = title_trainer.predict(title_test_dataset)
title_predicted_labels = np.argmax(title_predictions.predictions, axis=1)
title_true_labels = title_test_dataset["label"]

title_accuracy = accuracy_score(title_true_labels, title_predicted_labels)
print(f"Test Accuracy: {title_accuracy}")

print(classification_report(title_true_labels, title_predicted_labels, digits = 4))

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
contents_model = BertForSequenceClassification.from_pretrained("klue/bert-base", num_labels=2).to(device)

contents_trainer = Trainer(
    model=contents_model.to(device),
    args=training_args2,
    train_dataset=contents_train_dataset,
    eval_dataset=contents_val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

contents_trainer.train()

contents_predictions = contents_trainer.predict(contents_test_dataset)
contents_predicted_labels = np.argmax(contents_predictions.predictions, axis=1)
contents_true_labels = contents_test_dataset["label"]

contents_accuracy = accuracy_score(contents_true_labels, contents_predicted_labels)
print(f"Test Accuracy: {contents_accuracy}")

print(classification_report(contents_true_labels, contents_predicted_labels, digits = 4))

title_test_predictions = title_trainer.predict(title_test_dataset)
contents_test_predictions = contents_trainer.predict(contents_test_dataset)

title_probs = torch.softmax(torch.tensor(title_test_predictions.predictions), dim=1)
contents_probs = torch.softmax(torch.tensor(contents_test_predictions.predictions), dim=1)

combined_probs = (title_probs + contents_probs) / 2

final_predictions = torch.argmax(combined_probs, dim=1).numpy()

true_labels = title_test_dataset['label']

final_accuracy = accuracy_score(true_labels, final_predictions)
print(f"Soft Voting Test Accuracy: {final_accuracy}")



# 학습 모델 저장
contents_model_save_path = "contents_model(batch32_epoch10).pth"
torch.save(contents_model.state_dict(), contents_model_save_path)

title_model_save_path = "title_model(batch32_epoch10).pth"
torch.save(title_model.state_dict(), title_model_save_path)
