# import
import torch
import re
import numpy as np
import pandas as pd
import random
import transformers
import evaluate
import torch.optim as optim
import torch.nn.functional as F
from torch import nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from transformers import AutoModel, AutoTokenizer
from collections import Counter
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertModel
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, EarlyStoppingCallback
from transformers import DataCollatorWithPadding
from lime.lime_text import LimeTextExplainer
from collections import Counter



# data 불러오기
data = pd.read_excel("daum_news_crawling(new).xlsx", index_col = 0, engine = 'openpyxl')



# 보수/진보 메이저 언론사 분류
p_data = data[(data['source'] == '조선일보') | (data['source'] == '중앙일보') | (data['source'] == '동아일보')]
p_data = p_data.reset_index()

n_data = data[(data['source'] == '한겨레') | (data['source'] == '경향신문')]
n_data = n_data.reset_index()



# 각종 전처리
p_data_merge = p_data.drop_duplicates(subset = 'title').reset_index(drop = True)
p_data_merge = p_data.drop_duplicates(subset = 'contents').reset_index(drop = True)

n_data_merge = n_data.drop_duplicates(subset = 'title').reset_index(drop = True)
n_data_merge = n_data.drop_duplicates(subset = 'contents').reset_index(drop = True)

p_data_final = p_data_merge.loc[:, ['title', "contents"]]
p_data_final['label'] = 0

p_data_final['title'] = p_data_final['title'].fillna("")
p_data_final['contents'] = p_data_final['contents'].fillna("")

n_data_final = n_data_merge.loc[:, ['title', "contents"]]
n_data_final['label'] = 1

tqdm.pandas()

def clean_text(texts):
    review = texts
    review = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\♣\▲\ⓒ\■\[\]\“\”\☞\‘\’\▶\·\…\〃\<\>"]', '', review) #remove punctuation
    review = re.sub(r'<[^>]+>','',review)
    review = re.sub(r'\s+', ' ', review)
    review = re.sub(r'^\s+', '', review)
    review = re.sub(r'\s+$', '', review)
    review = re.sub(r'[A-Za-z]+', '', review)
    review = re.sub(r'[\u4e00-\u9fff]+', '', review)
    review = re.sub(r'\S+\s+기자', '', review)
    review = re.sub(r'\S+\s+선임기자', '', review)
    review = re.sub(r'\S+\s+군사전문기자', '', review)
    review = re.sub(r'\S+기자', '', review)
    review = re.sub(r'\S+선임기자', '', review)
    review = re.sub(r'\S+군사전문기자', '', review)
    return review

p_data_final['clean_contents'] = p_data_final['contents'].progress_apply(clean_text)
n_data_final['clean_contents'] = n_data_final['contents'].progress_apply(clean_text)

pattern = r'\<.*?\>|\[.*?\]'

p_data_final['title'] = p_data_final['title'].apply(lambda x: re.sub(pattern, '', x))
n_data_final['title'] = n_data_final['title'].apply(lambda x: re.sub(pattern, '', x))

remove_source = ['조선일보', '중앙일보', '동아일보', '한겨레', '경향신문', '오마이뉴스', '동아닷컴', '뉴시스', '중앙포토', 
                 '무단 전재 및 재배포 금지', '기사', '어제 못본', '명장면이 궁금하다면', '오늘의', '김상호', '절친이 되어 주세요', 
                 '신문구독주주신청', '페이스북카카오톡사설칼럼신문', '무단전재 및 재배포 금지', '박태근', '박성진', '유신모', 
                 '김재중', '박은경', '정욱식', '코인데스크', '도쿄박형준', '박지훈', '최현호', '디지털뉴스팀', '팩트체크페이스', 
                 '김진우', '박영환', '구독', '윤상호', '김민석', '박병수', '박민규', '이상훈', '경향포토', '단독', '사진',
                 '포토뉴스', '코멘터리', '사설', '동아광장', '속보', 'VIEW', '포토', '뉴스원샷', '르포', '퇴근길 한 컷', '팩트체크'
                 'view', '서소문사진관', 'Why', '뉴스분석', '포토사오정', '뉴스원샷', '사진', 'Focus', '사진관', '칼럼']

pattern = '|'.join(remove_source)
p_data_final['clean_contents'] = p_data_final['clean_contents'].str.replace(pattern, '', regex = True)
n_data_final['clean_contents'] = n_data_final['clean_contents'].str.replace(pattern, '', regex = True)
p_data_final['title'] = p_data_final['title'].str.replace(pattern, '', regex = True)
n_data_final['title'] = n_data_final['title'].str.replace(pattern, '', regex = True)

sum_data_final = pd.concat([p_data_final, n_data_final], axis=0)
sum_data_final.reset_index(inplace=True)
sum_data_final['date'] = pd.to_datetime(sum_data_final['date'], errors='coerce')

sum_data_final = sum_data_final[sum_data_final['title'].str.len() >= 10]
sum_data_final.reset_index(inplace=True)
sum_data_final.drop(['level_0', 'index'], axis=1, inplace=True)

# 분석 시기에 맞게 날짜를 조정하여 실행

p_data_c = sum_data_final[(sum_data_final['source'] == '조선일보') & (sum_data_final['date'] >= '2017. 1. 9.') & (sum_data_final['date'] <= '2017. 5. 8.')]
p_data_c = p_data_c.reset_index()

p_data_j = sum_data_final[(sum_data_final['source'] == '중앙일보') & (sum_data_final['date'] >= '2017. 1. 9.') & (sum_data_final['date'] <= '2017. 5. 8.')]
p_data_j = p_data_j.reset_index()

p_data_d = sum_data_final[(sum_data_final['source'] == '동아일보') & (sum_data_final['date'] >= '2017. 1. 9.') & (sum_data_final['date'] <= '2017. 5. 8.')]
p_data_d = p_data_d.reset_index()

n_data_h = sum_data_final[(sum_data_final['source'] == '한겨레') & (sum_data_final['date'] >= '2017. 1. 9.') & (sum_data_final['date'] <= '2017. 5. 8.')]
n_data_h = n_data_h.reset_index()

n_data_k = sum_data_final[(sum_data_final['source'] == '경향신문') & (sum_data_final['date'] >= '2017. 1. 9.') & (sum_data_final['date'] <= '2017. 5. 8.')]
n_data_k = n_data_k.reset_index()

model_name = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

label_dict = {}
for i, label in enumerate(set(sum_data_final["label"])):
    label_dict[label] = i

sum_data_final = sum_data_final[["title", "clean_contents", "label"]]
for i in label_dict:
    sum_data_final.loc[(sum_data_final['label'] == i), 'label'] = label_dict[i]

title_train_data, title_test_data = train_test_split(sum_data_final, test_size=0.1, random_state=42)
title_train_data.rename(columns={'title': 'context'}, inplace=True)
title_test_data.rename(columns={'title': 'context'}, inplace=True)

title_train_data = title_train_data[["context", "label"]]
title_test_data = title_test_data[["context", "label"]]

title_train_dataset = Dataset.from_pandas(title_train_data)
title_test_dataset = Dataset.from_pandas(title_test_data)

title_train_dataset = title_train_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)
title_test_dataset = title_test_dataset.map(lambda example: tokenizer(example['context'], padding=True, truncation=True, max_length=512), batched=True)

device = torch.device("cuda")

title_model_save_path = "title_model(batch32_epoch10).pth"

title_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
title_model.load_state_dict(torch.load(title_model_save_path))



# LIME 분석
explainer = LimeTextExplainer(class_names=["Conservative", "Progressive"])
c_list = list()
j_list = list()
d_list = list()
h_list = list()
k_list = list()

def lime_explanation(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
    outputs = title_model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1).detach().cpu().numpy()
    return probabilities

for i in tqdm(range(len(p_data_c["title"])), desc="Generating LIME Explanations", unit="text"):
    text_to_explain = p_data_c["title"][i]
    explanation = explainer.explain_instance(text_to_explain, lime_explanation, num_features=5)
    c_list.append(explanation.as_list())
c_values = []
for exp in c_list:
    c_tuple = min(exp, key=lambda x: x[1])
    c_value = c_tuple[0]
    c_values.append(c_value)
print(c_values)
word_counts = Counter(c_values)
sorted_word_counts = word_counts.most_common()
print(sorted_word_counts)

for i in tqdm(range(len(p_data_j["title"])), desc="Generating LIME Explanations", unit="text"):
    text_to_explain = p_data_j["title"][i]
    explanation = explainer.explain_instance(text_to_explain, lime_explanation, num_features=5)
    j_list.append(explanation.as_list())
j_values = []
for exp in j_list:
    j_tuple = min(exp, key=lambda x: x[1])
    j_value = j_tuple[0]
    j_values.append(j_value)
print(j_values)
word_counts = Counter(j_values)
sorted_word_counts = word_counts.most_common()
print(sorted_word_counts)

for i in tqdm(range(len(p_data_d["title"])), desc="Generating LIME Explanations", unit="text"):
    text_to_explain = p_data_d["title"][i]
    explanation = explainer.explain_instance(text_to_explain, lime_explanation, num_features=5)
    d_list.append(explanation.as_list())
d_values = []
for exp in d_list:
    d_tuple = min(exp, key=lambda x: x[1])
    d_value = d_tuple[0]
    d_values.append(d_value)
print(d_values)
word_counts = Counter(d_values)
sorted_word_counts = word_counts.most_common()
print(sorted_word_counts)

for i in tqdm(range(len(n_data_h["title"])), desc="Generating LIME Explanations", unit="text"):
    text_to_explain = n_data_h["title"][i]
    explanation = explainer.explain_instance(text_to_explain, lime_explanation, num_features=5)
    h_list.append(explanation.as_list())
h_values = []
for exp in h_list:
    h_tuple = max(exp, key=lambda x: x[1])
    h_value = h_tuple[0]
    h_values.append(h_value)
print(h_values)
word_counts = Counter(h_values)
sorted_word_counts = word_counts.most_common()
print(sorted_word_counts)

for i in tqdm(range(len(n_data_k["title"])), desc="Generating LIME Explanations", unit="text"):
    text_to_explain = n_data_k["title"][i]
    explanation = explainer.explain_instance(text_to_explain, lime_explanation, num_features=5)
    k_list.append(explanation.as_list())
k_values = []
for exp in k_list:
    k_tuple = max(exp, key=lambda x: x[1])
    k_value = k_tuple[0]
    k_values.append(k_value)
print(k_values)
word_counts = Counter(k_values)
sorted_word_counts = word_counts.most_common()
print(sorted_word_counts)
